% Overview of section
In the previous section, we described how \code{rfactor} transforms a Halide reduction to expose new data parallelism. Doing so requires synthezising the associative binary operator equivalent to the Halide \emph{update} definition being factored. In this section we describe how we do this synthesis.

% Set up problem, LUT solution
In some cases generating the equivalent associative operator is trivial (e.g. summation). In other cases it is not, especially when the function reduces onto multiple values (see the examples in Figure \ref{fig:suggraphs}). The \emph{inverse} problem is easier: Given an associative binary operator, it is straightforward to generate the Halide update definition that implements reduction by it. Therefore, if there were a \emph{small} finite set of associative binary operators, we could simply attempt to match the update definition being factored against each of them in turn. Halide already includes facilities for doing regex-like matching of expressions against patterns with wildcards. Unfortunately, programmers can use more meaningful associative binary operators than could reasonably be thought of ahead of time by a compiler author.

% Full synthesis solution
An alternative approach is to use program synthesis techniques~\cite{Solar-Lezama:2008:PSS:1714168, Torlak:2013:GSL:2509578.2509586} to synthesize the corresponding associative reduction at compile-time when the call to \code{rfactor} is made. This is intractably slow, and can increase compile times of Halide pipelines from seconds to hours.

% Our solution
We use a hybrid of the two approaches, amplified with a strategy for decomposing each synthesis problem into several simpler problems. Offline, we generate a \emph{large} finite table of \emph{elementary} associative binary operators and their identities. These are akin to primes -- they are the associative operators which cannot be decomposed into a combination of simpler associative binary operators. At compile-time, we decompose the given Halide update definition into a composition of simpler, lower-dimensional definitions in the same way, then search the table for the elementary operator corresponding to each. If consistent matches are found, we reassemble the results into a composite associative operator equivalent to the original update definition. In subsection~\ref{subsec:generation} we describe how we generate the table, and in subsection~\ref{subsec:decomposition} we describe the decomposition procedure.

\subsection{Generating Elementary Operators}
\label{subsec:generation}

We begin with an enumeration of all tuples of expression trees in two tuples of inputs $x$ and $y$. The operators used to form the inner nodes of tree are Halide's IR nodes (\code{*}, \code{+}, \code{-}, \code{min}, \code{max}, \code{\&\&}, \code{||}, \code{select}, \code{<}, etc). We can reject some classes of uninteresting expressions by excluding them from the enumeration altogether. We only generate trees which use both $x$ and $y$. For every commutative operator in the tree, we only generate subtrees where there are at least as many leaves on the left as the right (TODO: how do we account for this during matching?). We generate trees using a single generic constant $k$, rather than generating trees containing all possible constants as leaves. We do not generate trees that would be trivially simplified (For example, we do not generate subexpressions like $max(x_0, x_0)$). TODO: other constraints on enumeration

We then subject the expressions to a battery of tests so that only elementary associative operators remain. The tests are arranged in increasing order of expense, so that we can cheaply reject most expressions.

\begin{enumerate}
\item For an operator $f$, we construct the expressions $f(f(x, y), z)$ and $f(x, f(y, z))$, and substitute in TODO randomly-selected values for $x, y, z$. If the two expressions don't evaluate to the same thing, the expression is not associative and can be rejected.
\item We apply both the Halide simplifier and solver to the expression, which simplifies and canonicalizes it in certain ways (e.g. in a tree of commutative operations, $x_i$ is always to the left of $x_{i+1}$ if possible, and constants are always on the right). All inputs to the matching process will have gone through the same process, so if the expression was mutated by the simplifier or solver, then we will never expect see it as an input, and so it can be discarded.
\item We then reject operators which can be decomposed using the decomposition procedure in Subsection~\ref{subsec:decomposition}.
\item Finally, to \emph{prove} that the expression is associative, we ask Z3~\cite{DeMoura:2008:ZES:1792734.1792766} to verify that $\forall x, y, z, k  f(f(x, y), z) = f(x, f(y, z))$. If this proof succeeds, we ask Z3 to solve $\forall x, f(id, x) = x$ to synthesize the identity $id$.
\item TODO: are there filters I missed?
\end{enumerate}

In the table of expressions that this produces, $x$ represents the partial result being reduced onto, and $y$ is a wildcard which could match any expression. When matching, $x$ must also appear on the left-hand-side of the update definition. $y$ may depend on the reduction domain coordinates, but may not depend on the partial results. The constant $k$, if it exists, may match anything which neither depends on the reduction domain coordinates nor the partial result. For example, the update definition which separately computes the sum-of-squares of the even and odd values of some input \code{in} could be written as:

\code{f(in(r) \% 2) = f(in(r) \% 2) + pow(in(r), 2)}

This would match against the pattern $x + y$, where $x$ matches \code{f(in(r) \% 2)}, and $y$ matches \code{pow(in(r), 2)}.

We terminate our enumeration at trees with TODO leaves and TODO tuple components. This takes TODO hours and generates TODO elementary operators to match against. The practical limit on this is the number of operators in the resulting table, rather than the time taken to generate them. As the table size grows, matching takes more time, and the table of operators itself takes longer to compile and bloats the binary size of the Halide compiler. At the same time, the operators become more and more esoteric and improbable to encounter. There are however meaningful operators that we never reach because we terminate our enumeration. For example, we never generate fully unrolled 3x3 matrix multiplication. It is an elementary associative operator in 9 tuple elements, where every expression tree has 6 leaf nodes.

\subsection{Subgraph Decomposition}
\label{subsec:decomposition}

To talk about how we decompose reductions into elementary operators, we must first discuss how one might \emph{compose} elementary reductions. The simplest way to compose two reductions into higher-dimensional reductions is to compute them both of them at the same time independently. This means that reductions that are a concatenation of smaller independent associative reductions are also associative. For example:

\code{f() = \{f()[0] + in(r), f()[1] * in(r)\}}

is associative, because it is a composition of the following two associative reductions:

\code{f0() = f0() + in(r)}

\code{f1() = f1() * in(r)}

Secondly, if we have an associative operator in which two tuple components compute the same value, we can deduplicate it, reducing the dimensionality by one. The result is still associative. Therefore, we can prove a reduction is associative by duplicating one of the elements to break a dependency and then applying the rule above. Consider the case of two-dimensional argmin:

\begin{lstlisting}[caption={Two-dimensional argmin. The three tuple components are the minimum value, and its x and y coordinates.}]
f() = {min(f()[0], in(r.x, r.y)),
       select(f()[0] < in(r.x, r.y), f()[1], r.x),
       select(f()[0] < in(r.x, r.y), f()[2], r.y)}
\end{lstlisting}

All three tuple components depend on \code{f()[0]}, so we can't decompose this into independent reductions immediately. Let's duplicate the first tuple component to break the dependency:

\begin{lstlisting}[caption={Two-dimensional argmin with the value redundantly computed}]
f() = {min(f()[0], in(r.x, r.y)),
       select(f()[0] < in(r.x, r.y), f()[1], r.x),
       min(f()[2], in(r.x, r.y)),
       select(f()[2] < in(r.x, r.y), f()[3], r.y)}
\end{lstlisting}

There are no dependencies between the first two components and the last two. This is now a composition of two one-dimensional argmin operations. Two-dimensional argmin is therefore not an elementary associative reduction, but is rather the composition of two one-dimensional argmin reductions.

Consider the directed graph of dependencies between tuple components. If we repeatedly duplicate tuple components (vertices) to break dependencies in this way, in the limit, we partition the graph into the set of subgraphs reachable from each vertex. If each such subgraph is an associative reduction, then the original reduction is associative. See Figure~\ref{fig:subgraph} for several examples.

%The general way to apply these two rules to decompose an arbitrary reduction into its components is to construct the directed graph $G$ of dependencies between tuple components. There is a vertex $N_i$ for each tuple component $i$, and there is an edge from $N_i$ to $N_j$ whenever the value of tuple component $i$ depends on tuple component $j$. The two rules above can then be stated as:

%\begin{enumerate}
%\item If every connected component of $G$ is an associative reduction, then $G$ is an associative reduction.
%\item If we duplicate one of the nodes of $G$, partitioning its incoming edges arbitrarily between the two copies, then the resulting graph is associative iff $G$ is associative, as they compute the same thing.
%\end{enumerate}

% This is a shitty ``proof''
%Let the subgraph $S_i$ be all vertices and edges reachable from $N_i$. We can make a copy of $S_i$ as its own connnected component set apart from the rest of $G$ by the following procedure: Initialize $S_i$ to contain only a duplicate of $N_i$, with no incoming edges. Then repeatedly consider all edges that leave $S_i$, and make duplicates of the vertices they point to, where edges from vertices within $S_i$ point to the duplicate, and edges from vertices not in $S_i$ point to the original. This procedure terminates when $S_i$ contains all vertices reachable from $N_i$. At that point, there are no edges between $S_i$ and the rest of $G$, so it is its own connected component. Once we construct each such $S_i$, we can discard $G$, as everything it computes is also computed by one of the disconnected subgraphs. Similarly, we can discard subgraphs which are completely contained by some other subgraph. Therefore, if every subgraph $S_i$ not contained within some other subgraph is an associative reduction, then $G$ is an associative reduction. For examples of these graphs and their subgraphs for several reductions see Figure \ref{fig:subgraph}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{subgraphs}
\caption{Dependency graphs (in blue) of various Halide $update$ definitions and their subgraph decomposition (each subgraph is marked by a red box).}
\label{fig:subgraphs}
\end{figure}

% Merging results of decomposition
After finding the associative operator equivalent to each subgraph separately, we need to merge the results. If any of the subgraphs are non-associative (or we fail to find the equivalent binary associative operator or an identity), we terminate and return an error. If a vertex $N_i$ appears in multiple subgraphs, we need to ensure that the binary associative operators for that tuple element (including their identities) deduced from each subgraph is the same. If there is contradiction, we terminate and return an error. In the cases where the reduction is indeed associative, but we have failed to prove that fact and have thus refused to apply \code{rfactor}, the programmer must manually factor the reduction.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{system}
\caption{To generate the precomputed associative operator table, we synthesize expression tress and use Z3 prover to verify the associativity of an expression tree and to compute its identity. \code{rfactor} takes in Halide \emph{update} definition and the precomputed table to generate the factorized \emph{update} definition. If the directed dependency graph of the \emph{update} is decomposable, \emph{rfactor} reduces the problem into sub-problems which are solved separately and merges those results.}
\label{fig:system}
\end{figure}
