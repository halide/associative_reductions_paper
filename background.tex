Programmer defines \emph{algorithm} in Halide through a Halide \emph{function}. Halide \emph{function} consists of sequence of stages, and is the unit by which we schedule things. By default, each of these stages represents a perfectly-nested loop nests in which a single value of the \emph{function} is computed and stored in the innermost loop per iteration. Stages beyond the first are called \emph{update} stages, and are allowed to recursively refer to the function. Some of the loops are data parallel and are constrained to be race-condition free by syntactic restrictions. These are represented as \code{Var}s. The bounds of these loops are inferred by Halide using interval arithmetic. Some other loops may have user-specified bounds and a user-specified nesting order, and fewer syntactic restrictions on their use. These are known as \code{RVar}s, which comprise \code{RDom}s. \code{RVar}s are used to express reductions, scattering, scans, etc. Each of these loop types, \code{Var} and \code{RVar}, can be manipulated in various ways through Halide scheduling primitives: they can be tiled, unrolled, mutually interchanged, etc., provided that the nesting order of \code{RVar}s is respected. 

While \code{Var}s are safe to parallelize or vectorize by construction -- \code{Var}s represents the naturally data-parallel axes of an \emph{algorithm} -- , \code{RVar}s can be parallelized or vectorized if and only if Halide can prove that no race condition exists. This makes parallelizing or vectorizing stages that use only \code{RVar}s difficult. For example, consider the two-dimensional convolution blur kernel shown in Listing \ref{lst:blur_loopness}, which is easily parallelizable across \code{Var} $x$ and $y$. Histogram of an image (see Listing \ref{lst:histogram_loopness}), on the other hand, is harder to parallelize since its update stage only involves \code{RVar}s. In order to parallelize a reduction like histogram, one needs to be able to factorize it into slices that have no dependencies on each other.

Although there have been several works in automatic generation of parallel associative reductions from a serial reduction, most of those works assume an explicit associative binary reduction operator, which is not applicable to Halide. Since Halide does not support reduction using a binary operator as a first-class primitive, reduction in Halide is implemented through usage of non-data-parallel \code{RVar}s. For Halide to support parallel reductions, it needs to be able to deduce an equivalent binary associative reduction operator and its identity from a serial reduction expressed as an imperative Halide \emph{update}. 

Prior work by Morita et al.~\cite{Morita:2007:AIG:1250734.1250752} introduced automatic generations of divide-and-conquer parallel programs framework based on the third homomorphism theorem and derivation of weak-right inverse. However, it requires programmer to specify the leftwards and rightwards forms of the sequential function which may not be obvious to derive. Teo et al.~\cite{Teo:1997:DEP:266670.266697} proposed a method to synthesize parallel divide-and-conquer
programs from recurrence function (which has the closest form to Halide serial reduction) through induction. They first derive two equivalent pre-parallel forms of the recurrence function by applying some generalization rules and deduce the "unknowns", the intermediate and merge reduction functions, through induction of those two pre-parallel forms. Although, it can be applied to solve some complex recurrences, such as reduction of complex multiplication, it requires long derivation and is unable to deal with argmin, which requires non-trivial re-ordering of chain of select nodes during the induction steps. 

In recent years, there have been works on applying program synthesis to automatically generate parallel programs. Recent work by Smith et al.~\cite{Smith:2016:MPS:2908080.2908102} used program synthesis to automatically generate MapReduce-style distributed programs from input-output examples. \textsc{Sketch} (by Solar-Lezama ~\cite{Solar-Lezama:2008:PSS:1714168}) and \textsc{Rosette} (by Torlak et al.~\cite{Torlak:2013:GSL:2509578.2509586}) are two solver-aided programming languages with support for program synthesis. We asked \textsc{Sketch} and \textsc{Rosette} to synthesize the equivalent binary associative reduction operator (including its identity) for a Halide multiply reduction of 32-bit integer complex number. Unfortunately, they are too slow to be practical for runtime deduction during compilation. \textsc{Sketch} did not terminate, while \textsc{Rosette} finished in about 1 hour. For both cases, we limit the search space by giving \textsc{Sketch} and \textsc{Rosette} partially completed binary associative reduction operator of the form \code{op(op(x??, y??), op(x??, y??)))}. 

Superoptimization~\cite{Granlund:1992:EBU:143095.143146, Massalin:1987:SLS:36206.36194} searches for the shortest or most optimized way to compute a branch-free sequence of instructions, by exhaustively searching over a space of possible programs. These rewrites can then be turned into peephole optimizations in compilers. More recent work has used stochastic search~\cite{Phothilimthana:2016:SUS:2872362.2872387, Schkufza:2013:SS:2490301.2451150} and program synthesis~\cite{Lopes:2015:PCP:2737924.2737965} to find replacements for larger sequences of instructions. 

In this work, we find equivalent replacements of a Halide reduction through a combination of enumeration and synthesis; in addition, though our domain is more restricted, we search for larger replacements than most superoptimizers.

\begin{lstlisting}[caption={Convolution blur kernel is easily parallelizable across \code{Var} $x$ adn $y$.}, label={lst:blur_loopness}]
// First stage
for y in range(input.height()):
  for x in range(input.width()):
    blur[x][y] = 0
// Update stage
parallel for y in range(input.height()):
  parallel for x in range(input.width()):
    for ry in range(kernel.height()):
      for rx in range(kernel.width()):    
        blur[x][y] += 
          kernel[rx][ry]*input[x+rx-1][y+ry-1] 
\end{lstlisting}

\begin{lstlisting}[caption={Histogram of an image is hard to parallelize since its update stage does not involve \code{RVar}s.}, label={lst:histogram_loopness}]
// Serial version
// First stage
for x in range(256):
  hist[x] = 0
// Update stage
for ry in range(input.height()):
  for rx in range(input.width()):
    hist[clamp(int(input[rx][ry]), 0, 255)] += 1
\end{lstlisting}
