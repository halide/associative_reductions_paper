\subsubsection{Halide}

Halide functions have a sequence of stages. Before scheduling, each of these stages represents a perfect loop nest whose body computes and stores a single value of the function. Stages beyond the first are called ``update'' stages, and are permitted to recursively refer to the function. Some of the loops are data parallel and made race-condition free by syntactic restrictions. These are represented as Vars. The bounds of these loops are inferred by Halide using interval arithmetic. Other loops have user-specified bounds and a user-specified nesting order, and fewer syntactic restrictions on their use. These are known as RVars, which comprise RDoms. They are used to express reductions, scattering, scans, etc.

Each type of loop can be manipulated in various ways by Halide's scheduling primitives. They can be tiled, unrolled, mutually interchanged (provided RVar nesting order is respected) etc. The difference: Vars are safe to parallelize over by construction - they represent the naturally data-parallel axes of a problem, whereas RVars cannot be parallelized or vectorized unless Halide can prove that no race condition exists. This makes it hard to find ways to parallelize or vectorize definitions that use only RVars.

Examples (convolution - x and y are Vars, iteration over kernel is RVars easy to parallelize, histogram is only RVars - hard to parallelize)

So Halide does not have the idea of a reduction using a binary operator as a first-class primitive. It merely has these non-data-parallel RVars which can be used to implement reductions. Much of the problem we need to solve is deducing the associative binary operator that corresponds to one of Halide's update stages.

How does existing work not quite solve it? \\

Work on synthesizing parallel (data parallelism + vectorization) reductions out of serial code. \\

Work on deducing associative operators: \\
\begin{enumerate}
	\item Deducing the map/reduce operators via theory of list homomorphisms + weak right inverse by Morita et al \cite{Morita:2007:AIG:1250734.1250752}. Con: User have to specify the leftwards and rightwards forms of the sequential function which may not be obvious to derive.
	\item Using induction to derive the parallel form of recurrence function by Chin et al \cite{Teo:1997:DEP:266670.266697} (This is the closest to Halide update definition). Steps: derive 2 pre-parallel funcs through generalization and deduce the "unknowns" (initial/final reduction functions) through induction of those two funcs. Seems promising -> can solve for intermediate/combiner for complex multiplication, albeit through long derivation. Argmin is tricky though: need to be able to re-order chain of selects, which is not trivial, during the induction steps, e.g. transforming select(min(f(x0)[0], g(y)) < g(x), select(f(x)[0] < g(y), f(xs)[1], y), x) into select(f(xs)[0] < min(g(x), g(y)), f(xs)[1], select(g(x) < g(y), x, y)).
	\item Deducing the map/reduce operators via (backward) synthesis by Smith et al \cite{Smith:2016:MPS:2908080.2908102}. Not sure how the whole pipeline actually works. They claims they can synthesize the right map/reduce ops given some input/output examples. Table 2 listed the components they used. I would assume they search for combination of components until they get the right one. The time it takes to synthesize the map/reduce seems to be okay, but they said they used only small inputs (3-8 samples) to get the benchmark data. \\
\end{enumerate}

We are inspired by synthesis: use the original update definition (serial version) as *assertion* and 'sketch' for the holes (intermediate function, merge function, initial value for the intermediate function). We tried "Sketch" (by Solar-Lezama \cite{Solar-Lezama:2008:PSS:1714168}) and "Rosette" (by Torlak et al \cite{Torlak:2013:GSL:2509578.2509586}) to synthesize intermediate/mergelreduction functions for the rfactor of 32-bit integer simplex complex multiplication. "Sketch" failed to find one within reasonable time; Rosette takes about one hour (with some constraints: x are fixed only to appear in RHS and depth is set to 1). Backward synthesis -> really slow, which inspires us to do forward synthesis.\\

Work on superoptimization (in particular, Regehr et al and Mangpo's ASPLOS16 paper, but also earlier work)

