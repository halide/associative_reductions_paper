Programmers define an \emph{algorithm} in Halide using a graph of Halide \emph{function}s, which each consist of a sequence of stages. These stages are the unit on which scheduling occurs. Each stage represents a perfectly-nested loop in which a single value of the \emph{function} is computed and stored in the innermost loop per iteration. Stages after the first are called \emph{update} stages, and are allowed to recursively refer to the function. Some of the loops are data parallel and are constrained to be race-condition free by syntactic restrictions. These data-parallel loops iterate over variables called \code{Var}s. The bounds of these loops are inferred by Halide using interval arithmetic. Other loops may have user-specified bounds and a user-specified nesting order, and fewer syntactic restrictions on their use. These loop variables are known as \code{RVar}s (for reduction variables), which together define a reduction domain or \code{RDom}. \code{RVar}s are used to express reductions, scattering, scans, etc. Each of these loop types, defined by \code{Var}s and \code{RVar}s, can be manipulated in various ways using Halide's scheduling primitives: they can be tiled, unrolled, mutually interchanged, etc., provided that the nesting order of \code{RVar}s is respected.

While \code{Var}s are safe to parallelize or vectorize by construction -- \code{Var}s represents the naturally data-parallel axes of an \emph{algorithm} -- \code{RVar}s can be parallelized or vectorized if and only if Halide can prove that no race condition exists. This makes parallelizing or vectorizing stages that use only \code{RVar}s difficult. For example, consider the two-dimensional convolution shown in Listing~\ref{lst:blur_loopness}, which is parallelizable across \code{Var} $x$ and $y$. Computing the histogram of an image (see Listing~\ref{lst:histogram_loopness}) is harder to parallelize since its update stage only involves \code{RVar}s.

\begin{lstlisting}[float,caption={Pseudocode for convolution. This algorithm reduces over \code{rx} and \code{ry} and is data-parallel over \code{x} and \code{y}. In the Halide source, \code{rx} and \code{ry} would be \code{RVar}s in a two-dimensional \code{RDom}. \code{x} and \code{y} would be \code{Var}s.}, label={lst:blur_loopness}]
// First stage
for y:
 for x:
   blur(x, y) = 0
// Second "update" stage
for y:
 for x:
  for ry:
   for rx:
    blur(x, y) += k(rx, ry) * in(x-rx, y-ry);
\end{lstlisting}
\begin{lstlisting}[float,caption={Computing the histogram of an image is hard to parallelize in Halide, since its update stage would be expressed with serial \code{RVar}s.}, label={lst:histogram_loopness}]
// First stage
for x:
 hist(x) = 0
// Update stage
for ry:
 for rx:
  hist(input(rx, ry)) += 1
\end{lstlisting}

Although much prior work has explored automatic generation of parallel associative reductions from a serial reduction, most work requires that an explicit associative binary reduction operator is given. One widely-deployed example is OpenMP~\cite{Dagum:1998:OIA:615255.615542} which has a first-class parallel reduction construct that takes one of some number of primitive reduction operators. Cilk~\cite{Blumofe:1995:CEM:209936.209958} additionally supports user-specified reduction operators. This approach is not applicable to Halide. Reductions in Halide are implemented through implicit serial loops over \code{RVar}s. The reduction operator is never explicitly stated. For Halide to support parallel reductions, it needs to be able to deduce an equivalent binary associative reduction operator and its identity from a serial reduction expressed as an imperative Halide \emph{update}.

LLVM~\cite{Lattner:2004:LCF:977395.977673} can automatically recognize a small set of associative reductions for the purposes of auto-vectorization. However, it has the usual problems associated with auto-vectorization -- small changes to the program can cause auto-vectorization to fail for reasons mysterious to the programmer, and only simple loops reliably auto-vectorize. Halide's philosophy is one of explicit control. Nevertheless, Halide compiles to LLVM bitcode, and so for simple reductions we will benefit from this auto-vectorization even if the programmer does not employ \code{rfactor}.

Prior work by Morita et al.~\cite{Morita:2007:AIG:1250734.1250752} introduced automatic generation of divide-and-conquer parallel programs using a framework based on the third homomorphism theorem and derivation of weak-right inverse. However, it requires that programmers specify the leftwards and rightwards forms of the sequential function. Teo et al.~\cite{Teo:1997:DEP:266670.266697} proposed a method to synthesize parallel divide-and-conquer
programs from a recurrence function (which is similar in form to a Halide serial reduction) through induction. They first derive two equivalent pre-parallel forms of the recurrence function by applying some generalization rules and deduce the intermediate and merge reduction functions through induction on those two pre-parallel forms. Although it can be applied to solve some complex recurrences, such as the product of a list of complex numbers, the technique is slow, and is unable to deal with reductions like \code{argmin}, which require non-trivial re-ordering of the chain of conditionals during the induction steps.

Recent work has applied program synthesis, which automatically discovers executable code based on user intent derived from examples or other constraints, to generate parallel programs. Smith et al.~\cite{Smith:2016:MPS:2908080.2908102} used program synthesis to automatically generate MapReduce-style distributed programs from input-output examples. \textsc{Sketch}~\cite{Solar-Lezama:2008:PSS:1714168} and \textsc{Rosette}~\cite{Torlak:2013:GSL:2509578.2509586} are two solver-aided programming languages with support for program synthesis.  MSL~\cite{Xu:2014:MSE:2683593.2683628} is a synthesis-based language for distributed implementations that can derive many details of the distributed implementation from serial specifications. We tried \textsc{Sketch} and \textsc{Rosette} and found them too slow to apply directly at compile time.

Superoptimization~\cite{Granlund:1992:EBU:143095.143146, Massalin:1987:SLS:36206.36194} searches for the shortest or most optimized way to compute a branch-free sequence of instructions by exhaustively searching over a space of possible programs. These rewrites can then be turned into peephole optimizations in compilers. More recent work has used stochastic search~\cite{Phothilimthana:2016:SUS:2872362.2872387, Schkufza:2013:SS:2490301.2451150} and program synthesis~\cite{Lopes:2015:PCP:2737924.2737965} to find replacements for larger sequences of instructions.
In this work, we use a combination of enumeration and synthesis; in addition, though our domain is more restricted, we search for larger replacements than most superoptimizers.
