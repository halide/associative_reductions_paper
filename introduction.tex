Halide \cite{Ragan-Kelley:2013:HLC:2491956.2462176} is important, provides separation of algorithm and schedule. Ability to try various schedules with guaranteed correctness and consistency: different schedules are guaranteed to produce the same output as long they define the same computation. \\

One important class of problems: associative reduction. This can be as simple as single-dimensional sum or some complex multidimensional associative ops (see Figure 1). One way to optimize performance in associative reduction: split into smaller chunks of works, compute them separately, and merge the partial result. Associative property allows such optimization. \\

Figure:

\begin{lstlisting}[
caption = {make the case that they're both associative reductions, but it's not obvious what the binary operator is from the code for the second one}]
Summation (easy example)

Func out;
out() = 0;
RDom r(0, input.width());
out() = out() + input(r.x);

The complex number with the greatest magnitude, 
and its location (hard example)

Func out;
out() = {0, 0, 0};
RDom r(0, input.width(), 0, input.height());
Expr real = input(r.x, r.y)[0];
Expr imag = input(r.x, r.y)[1];
Expr mag = real * real + imag * imag;
Expr best_mag = out()[0] * out()[0] 
	+ out()[1] * out()[1];
Expr c = mag > best_mag;
out() = {select(c, real, out()[0]),
         select(c, imag, out()[1]),
         select(c, r.x, out()[2]),
         select(c, r.y, out()[3])};
\end{lstlisting}

However, Halide did not support parallel or *vectorized* reductions till now without changing algorithm (which fails to deliver core promise of language in which schedule and algorithm should be separate and not affect each other). We present a Halide scheduling primitive (DAG transformation) that creates a new data parallelizable/vectorizable axis out of a reduction. \\

rfactor splits an update (**associative update**) into an intermediate which computes the partial results and a new update definition which merges the partial results -> creates separate copies of the reduction dimension which exposes a new data parallelism (parallelizable + vectorizable), and a second stage that combines those partial results. Combined with other Halide scheduling directives, such as split, this allows Halide to represent a broader class of schedules, including parallel associative reduction. 

<Insert some code snippet of pipeline produced by rfactor, including performance numbers for it -> serial, hand-rolled, using rfactor> \\

\begin{lstlisting}[caption={Histogram loop-ness generated by: serial version, hand-rollled parallel associative reduction version, and rfactor parallel associative reduction version}]
// Serial version
for x in range(256):
  hist[x] = 0
for ry in range(input.height()):
  for rx in range(input.width()):
    hist[clamp(int(input[rx][ry]), 0, 255)] += 1

// Hand-rolled version: compute histogram along 
// the x-scanline for each y in parallel, then 
// reduce the partial results
for u in range(input.height()):
  intm[u] = 0
parallel for u in range(input.height()):
  for rx in range(input.width()):
    intm[clamp(int(input[rx][u]), 0, 255)] += 1
for x in range(256):
  hist[x] = 0
for ry in range(input.height()):
  hist[clamp(int(intm[ry]), 0, 255)] += 1

// rfactor version : compute histogram along  
// the x-scanline for each y in parallel, then  
// reduce the partial results

for u in range(input.height()):
  intm[u] = 0
parallel for u in range(input.height()):
  for rx in range(input.width()):
    intm[clamp(int(input[rx][u]), 0, 255)] += 1
for x in range(256):
  hist[x] = 0
for ry in range(input.height()):
  hist[clamp(int(intm[ry]), 0, 255)] += 1
\end{lstlisting}

Other benefits: code reduction, supports purity/separation of algorithm and schedule, and portability, which is especially important for auto-scheduling \cite{Mullapudi:2016:ASH:2897824.2925952}. \\
