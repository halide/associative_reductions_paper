Performance results using rfactor (overall speedup): 
2D histogram, 4D argmin, complex multiplication, saturating addition, dot product (vectorize inner or parallelize outer), matrix multiply vs. the serial versions, convolution with large kernels (vectorizing the inner reduction?). \\

\begin{table*}[h!]
\caption{Benchmark results: serial vs. parallel \code{rfactor}}
\label{tab:table}
\centering
%\setlength\tabcolsep{3.5pt} % default value: 6pt
\begin{center}
\begin{tabular}{p{5cm}ddd}
\toprule
\multicolumn{1}{C{5cm}}{Benchmark} & \multicolumn{1}{C{2cm}}{Serial (ms)} & \multicolumn{1}{C{2cm}}{\code{rfactor} (ms)} & \multicolumn{1}{C{2cm}}{Speed-up}\\
\midrule 
2D histogram 							&  &  & \\
4D argmin 								&  &  & \\
Complex multiplication 			&  &  & \\
Saturating addition	 				&  &  & \\
Dot-product 							&  &  & \\
Square matrix multiplication 	&  &  & \\
(size??)x(size??) convolution  &  &  & \\
\bottomrule 
\end{tabular}
\end{center}
\label{default}
\end{table*}

Synthetic functions (also to show limitations): approximating 128-bit add with 2 64-bit integers -> z3 cannot prove that it is associative, although the max/min forms are provable with z3. \\

Limitations: we need an identity, symmetric intermediate and merge functions: they should be of the same form, constrained by the look-up table (we can only match to whatever are in the table -> whatever z3 can prove to be associative) + whatever we thought to generate. Some associative ops (e.g. 4x4 matrix multiply) are just expensive to generate. Technically it's doable, provided we limit the ops to addition and multiplication and restricting the variables involved in the expr to be unique (no repeats) during lookup table generation. Other ops may not be covered in our search (e.g. one that exploits a special property unique to some large constant). \\

Real-world stuff: same code that can be simplified (reducing number of lines of code, etc) when using rfactor \\

Performance of generation/search/synthesis. <TODO: Not sure what this is about? time taken when doing the matching? how many associative operations can be generated within some hours? > \\

Case study of importance of "code reduction"? <TODO: Not sure what this is about? SK: I think the idea is that without the approach in this paper, you have to write more code (= more bugs) but also that you have to write arch-specific code (if you want to do parallel reduction on one arch and not the other) > \\ 
